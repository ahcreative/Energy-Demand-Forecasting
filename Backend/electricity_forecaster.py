# -*- coding: utf-8 -*-
"""electricity_forecaster.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uq9sEFPlnNzXKnsJJjB2QU6tLAXph71u
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
import xgboost as XGBoost
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
import joblib
import warnings
warnings.filterwarnings('ignore')

class ElectricityDemandForecaster:
    """
    A class for forecasting electricity demand using various models
    """
    def __init__(self, data_path='processed_energy_data.csv'):
        """
        Initialize the forecaster with data path
        """
        self.data_path = data_path
        self.data = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.models = {}
        self.predictions = {}
        self.baseline_predictions = None
        self.metrics = {}

    def load_data(self):
        """
        Load and preprocess the data
        """
        print("Loading data...")
        self.data = pd.read_csv(self.data_path)

        # Convert datetime columns
        if 'local_time' in self.data.columns:
            self.data['local_time'] = pd.to_datetime(self.data['local_time'])

        # Ensure we have demand and necessary features
        if 'demand' not in self.data.columns:
            raise ValueError("Dataset must contain 'demand' column")

        print(f"Data loaded: {self.data.shape[0]} rows, {self.data.shape[1]} columns")
        return self.data

    def prepare_features(self, city=None, forecast_horizon=24):
        """
        Prepare features for modeling

        Parameters:
        -----------
        city : str, optional
            Filter data for specific city
        forecast_horizon : int, default=24
            Number of hours ahead to forecast

        Returns:
        --------
        X_features, y_target : pd.DataFrame, pd.Series
            Features and target for modeling
        """
        print(f"Preparing features for {'all cities' if not city else city}...")

        # Filter by city if specified
        if city:
            data = self.data[self.data['city'] == city.lower()].copy()
        else:
            data = self.data.copy()

        # Sort by time
        data = data.sort_values('local_time')

        # Extract features
        feature_cols = []

        # Weather features
        weather_cols = ['temperature', 'humidity', 'windSpeed', 'pressure',
                        'precipIntensity', 'cloudCover']
        for col in weather_cols:
            if col in data.columns:
                feature_cols.append(col)

        # Temporal features
        temporal_cols = ['hour', 'day_of_week', 'month', 'is_weekend', 'is_peak_hour']
        for col in temporal_cols:
            if col in data.columns:
                feature_cols.append(col)

        # Create lag features
        lag_periods = [1, 24, 48, 168]  # Previous hour, day, 2 days, week
        for lag in lag_periods:
            data[f'demand_lag_{lag}'] = data['demand'].shift(lag)
            feature_cols.append(f'demand_lag_{lag}')

        # Create rolling average features
        windows = [6, 12, 24, 48]
        for window in windows:
            data[f'demand_rolling_{window}'] = data['demand'].rolling(window=window).mean()
            feature_cols.append(f'demand_rolling_{window}')

        # Drop rows with NaN values (from lagging and rolling features)
        data = data.dropna()

        # Create target variable (future demand)
        data['target_demand'] = data['demand'].shift(-forecast_horizon)

        # Drop rows where target is NaN
        data = data.dropna(subset=['target_demand'])

        # Select features and target
        X = data[feature_cols]
        y = data['target_demand']

        print(f"Features prepared: {len(feature_cols)} features, {X.shape[0]} samples")
        return X, y

    def train_test_split_by_time(self, X, y, test_size=0.2):
        """
        Split data into train/test sets chronologically
        """
        # Get the index values where the split should happen
        split_idx = int(len(X) * (1 - test_size))

        # Split the data
        self.X_train, self.X_test = X.iloc[:split_idx], X.iloc[split_idx:]
        self.y_train, self.y_test = y.iloc[:split_idx], y.iloc[split_idx:]

        print(f"Train set: {self.X_train.shape[0]} samples, Test set: {self.X_test.shape[0]} samples")
        return self.X_train, self.X_test, self.y_train, self.y_test

    def scale_features(self):
        """
        Scale numerical features
        """
        # Create a scaler
        scaler = StandardScaler()

        # Get numerical columns
        num_cols = self.X_train.select_dtypes(include=['number']).columns

        # Fit and transform training data
        self.X_train[num_cols] = scaler.fit_transform(self.X_train[num_cols])

        # Transform test data
        self.X_test[num_cols] = scaler.transform(self.X_test[num_cols])

        # Save the scaler
        self.scaler = scaler
        self.num_cols = num_cols

        return self.X_train, self.X_test

    def create_baseline_forecast(self):
        """
        Create naive baseline forecast (previous day's same hour)
        """
        # Baseline prediction: use the same hour from the previous day (lag = 24)
        if 'demand_lag_24' in self.X_test.columns:
            self.baseline_predictions = self.X_test['demand_lag_24']

            # Calculate baseline metrics
            baseline_mae = mean_absolute_error(self.y_test, self.baseline_predictions)
            baseline_rmse = np.sqrt(mean_squared_error(self.y_test, self.baseline_predictions))
            baseline_mape = mean_absolute_percentage_error(self.y_test, self.baseline_predictions) * 100

            self.metrics['baseline'] = {
                'MAE': baseline_mae,
                'RMSE': baseline_rmse,
                'MAPE': baseline_mape
            }

            print(f"Baseline MAE: {baseline_mae:.2f}, RMSE: {baseline_rmse:.2f}, MAPE: {baseline_mape:.2f}%")
        else:
            print("Warning: Cannot create baseline forecast without 24-hour lag feature")

    def train_linear_regression(self):
        """
        Train a linear regression model
        """
        print("Training Linear Regression model...")
        model = LinearRegression()
        model.fit(self.X_train, self.y_train)

        # Make predictions
        predictions = model.predict(self.X_test)

        # Calculate metrics
        mae = mean_absolute_error(self.y_test, predictions)
        rmse = np.sqrt(mean_squared_error(self.y_test, predictions))
        mape = mean_absolute_percentage_error(self.y_test, predictions) * 100

        self.models['linear'] = model
        self.predictions['linear'] = predictions
        self.metrics['linear'] = {
            'MAE': mae,
            'RMSE': rmse,
            'MAPE': mape
        }

        print(f"Linear Regression - MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}%")
        return model, predictions

    def train_random_forest(self, param_grid=None):
        """
        Train a Random Forest model with optional grid search
        """
        print("Training Random Forest model...")

        if param_grid is None:
            param_grid = {
                'n_estimators': [50, 100],
                'max_depth': [10, 20, None],
                'min_samples_split': [2, 5, 10]
            }

        # Initialize model
        base_model = RandomForestRegressor(random_state=42)

        # Time series cross-validation
        tscv = TimeSeriesSplit(n_splits=3)

        # Grid search with time series CV
        grid_search = GridSearchCV(
            estimator=base_model,
            param_grid=param_grid,
            cv=tscv,
            scoring='neg_mean_squared_error',
            n_jobs=-1
        )

        # Fit grid search
        grid_search.fit(self.X_train, self.y_train)

        # Get best model
        best_model = grid_search.best_estimator_
        print(f"Best RF parameters: {grid_search.best_params_}")

        # Make predictions
        predictions = best_model.predict(self.X_test)

        # Calculate metrics
        mae = mean_absolute_error(self.y_test, predictions)
        rmse = np.sqrt(mean_squared_error(self.y_test, predictions))
        mape = mean_absolute_percentage_error(self.y_test, predictions) * 100

        self.models['random_forest'] = best_model
        self.predictions['random_forest'] = predictions
        self.metrics['random_forest'] = {
            'MAE': mae,
            'RMSE': rmse,
            'MAPE': mape
        }

        print(f"Random Forest - MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}%")
        return best_model, predictions

    def train_gradient_boosting(self, param_grid=None):
        """
        Train a Gradient Boosting model with optional grid search
        """
        print("Training Gradient Boosting model...")

        if param_grid is None:
            param_grid = {
                'n_estimators': [50, 100],
                'learning_rate': [0.01, 0.1],
                'max_depth': [3, 5]
            }

        # Initialize model
        base_model = GradientBoostingRegressor(random_state=42)

        # Time series cross-validation
        tscv = TimeSeriesSplit(n_splits=3)

        # Grid search with time series CV
        grid_search = GridSearchCV(
            estimator=base_model,
            param_grid=param_grid,
            cv=tscv,
            scoring='neg_mean_squared_error',
            n_jobs=-1
        )

        # Fit grid search
        grid_search.fit(self.X_train, self.y_train)

        # Get best model
        best_model = grid_search.best_estimator_
        print(f"Best GB parameters: {grid_search.best_params_}")

        # Make predictions
        predictions = best_model.predict(self.X_test)

        # Calculate metrics
        mae = mean_absolute_error(self.y_test, predictions)
        rmse = np.sqrt(mean_squared_error(self.y_test, predictions))
        mape = mean_absolute_percentage_error(self.y_test, predictions) * 100

        self.models['gradient_boosting'] = best_model
        self.predictions['gradient_boosting'] = predictions
        self.metrics['gradient_boosting'] = {
            'MAE': mae,
            'RMSE': rmse,
            'MAPE': mape
        }

        print(f"Gradient Boosting - MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}%")
        return best_model, predictions

    def train_xgboost(self, param_grid=None):
        """
        Train an XGBoost model with optional grid search
        """
        print("Training XGBoost model...")

        try:
            from xgboost import XGBRegressor

            if param_grid is None:
                param_grid = {
                    'n_estimators': [50, 100],
                    'learning_rate': [0.01, 0.1],
                    'max_depth': [3, 5],
                    'subsample': [0.8, 1.0]
                }

            # Initialize model
            base_model = XGBRegressor(random_state=42)

            # Time series cross-validation
            tscv = TimeSeriesSplit(n_splits=3)

            # Grid search with time series CV
            grid_search = GridSearchCV(
                estimator=base_model,
                param_grid=param_grid,
                cv=tscv,
                scoring='neg_mean_squared_error',
                n_jobs=-1
            )

            # Fit grid search
            grid_search.fit(self.X_train, self.y_train)

            # Get best model
            best_model = grid_search.best_estimator_
            print(f"Best XGB parameters: {grid_search.best_params_}")

            # Make predictions
            predictions = best_model.predict(self.X_test)

            # Calculate metrics
            mae = mean_absolute_error(self.y_test, predictions)
            rmse = np.sqrt(mean_squared_error(self.y_test, predictions))
            mape = mean_absolute_percentage_error(self.y_test, predictions) * 100

            self.models['xgboost'] = best_model
            self.predictions['xgboost'] = predictions
            self.metrics['xgboost'] = {
                'MAE': mae,
                'RMSE': rmse,
                'MAPE': mape
            }

            print(f"XGBoost - MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}%")
            return best_model, predictions

        except ImportError:
            print("XGBoost not available. Skipping XGBoost training.")
            return None, None

    def train_lstm(self, lookback=24, epochs=20, batch_size=32):
        """
        Train an LSTM neural network model
        """
        print("Training LSTM model...")

        try:
            # Prepare data for LSTM (reshape to [samples, time steps, features])
            X_train_lstm = self.X_train.values
            X_test_lstm = self.X_test.values

            # Build model
            model = Sequential()
            model.add(LSTM(units=50, return_sequences=True,
                           input_shape=(1, X_train_lstm.shape[1])))
            model.add(Dropout(0.2))
            model.add(LSTM(units=50))
            model.add(Dropout(0.2))
            model.add(Dense(units=1))

            # Compile model
            model.compile(optimizer='adam', loss='mean_squared_error')

            # Reshape data for LSTM [samples, timesteps, features]
            X_train_lstm = X_train_lstm.reshape((X_train_lstm.shape[0], 1, X_train_lstm.shape[1]))
            X_test_lstm = X_test_lstm.reshape((X_test_lstm.shape[0], 1, X_test_lstm.shape[1]))

            # Train model
            history = model.fit(
                X_train_lstm, self.y_train.values,
                epochs=epochs,
                batch_size=batch_size,
                validation_split=0.1,
                verbose=0
            )

            # Make predictions
            predictions = model.predict(X_test_lstm).flatten()

            # Calculate metrics
            mae = mean_absolute_error(self.y_test, predictions)
            rmse = np.sqrt(mean_squared_error(self.y_test, predictions))
            mape = mean_absolute_percentage_error(self.y_test, predictions) * 100

            self.models['lstm'] = model
            self.predictions['lstm'] = predictions
            self.metrics['lstm'] = {
                'MAE': mae,
                'RMSE': rmse,
                'MAPE': mape
            }

            print(f"LSTM - MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}%")
            return model, predictions, history

        except:
            print("Error training LSTM model. Skipping.")
            return None, None, None

    def train_sarima(self, city, order=(1,1,1), seasonal_order=(1,1,1,24)):
        """
        Train a SARIMA model for a specific city
        Note: SARIMA is trained on a single time series (one city)
        """
        print(f"Training SARIMA model for {city}...")

        try:
            # Filter data for the city
            city_data = self.data[self.data['city'] == city.lower()].copy()

            # Sort by time
            city_data = city_data.sort_values('local_time')

            # Get demand series
            demand_series = city_data['demand']

            # Split into train/test
            split_idx = int(len(demand_series) * 0.8)
            train_series = demand_series.iloc[:split_idx]
            test_series = demand_series.iloc[split_idx:]

            # Train SARIMA model
            model = SARIMAX(
                train_series,
                order=order,
                seasonal_order=seasonal_order
            )

            model_fit = model.fit(disp=False)

            # Forecast
            forecast = model_fit.forecast(steps=len(test_series))

            # Calculate metrics
            mae = mean_absolute_error(test_series, forecast)
            rmse = np.sqrt(mean_squared_error(test_series, forecast))
            mape = mean_absolute_percentage_error(test_series, forecast) * 100

            self.models[f'sarima_{city}'] = model_fit
            self.predictions[f'sarima_{city}'] = forecast
            self.metrics[f'sarima_{city}'] = {
                'MAE': mae,
                'RMSE': rmse,
                'MAPE': mape
            }

            print(f"SARIMA for {city} - MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}%")
            return model_fit, forecast

        except:
            print(f"Error training SARIMA model for {city}. Skipping.")
            return None, None

    # Continuation of ElectricityDemandForecaster class

    def train_ensemble(self, ensemble_type='averaging'):
        """
        Create an ensemble model

        Parameters:
        -----------
        ensemble_type : str
            Type of ensemble: 'averaging', 'stacking'
        """
        print(f"Creating {ensemble_type} ensemble...")

        # Check if we have models to ensemble
        if len(self.predictions) < 2:
            print("Need at least 2 models for ensembling. Skipping.")
            return None, None

        if ensemble_type == 'averaging':
            # Simple averaging of model predictions
            ensemble_pred = np.mean([
                self.predictions[model_name]
                for model_name in self.predictions
                if model_name != 'baseline' and 'sarima' not in model_name
            ], axis=0)

            # Calculate metrics
            mae = mean_absolute_error(self.y_test, ensemble_pred)
            rmse = np.sqrt(mean_squared_error(self.y_test, ensemble_pred))
            mape = mean_absolute_percentage_error(self.y_test, ensemble_pred) * 100

            self.predictions['ensemble_avg'] = ensemble_pred
            self.metrics['ensemble_avg'] = {
                'MAE': mae,
                'RMSE': rmse,
                'MAPE': mape
            }

            print(f"Ensemble (Averaging) - MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}%")
            return 'ensemble_avg', ensemble_pred

        elif ensemble_type == 'stacking':
            # Stacking: use a meta-model on the predictions of base models

            # Get base model predictions for training data
            base_preds_train = {}
            base_preds_test = {}

            for model_name, model in self.models.items():
                if model_name != 'baseline' and 'sarima' not in model_name:
                    # Get predictions for training data
                    if model_name == 'lstm':
                        # For LSTM, reshape data first
                        X_train_lstm = self.X_train.values.reshape(
                            (self.X_train.shape[0], 1, self.X_train.shape[1])
                        )
                        base_preds_train[model_name] = model.predict(X_train_lstm).flatten()
                    else:
                        base_preds_train[model_name] = model.predict(self.X_train)

                    # Store test predictions
                    base_preds_test[model_name] = self.predictions[model_name]

            # Create meta-features
            meta_features_train = np.column_stack([
                base_preds_train[model_name] for model_name in base_preds_train
            ])

            meta_features_test = np.column_stack([
                base_preds_test[model_name] for model_name in base_preds_test
            ])

            # Train meta-model (Ridge regression)
            meta_model = Ridge(alpha=1.0)
            meta_model.fit(meta_features_train, self.y_train)

            # Make predictions with meta-model
            ensemble_pred = meta_model.predict(meta_features_test)

            # Calculate metrics
            mae = mean_absolute_error(self.y_test, ensemble_pred)
            rmse = np.sqrt(mean_squared_error(self.y_test, ensemble_pred))
            mape = mean_absolute_percentage_error(self.y_test, ensemble_pred) * 100

            self.models['ensemble_stack'] = meta_model
            self.predictions['ensemble_stack'] = ensemble_pred
            self.metrics['ensemble_stack'] = {
                'MAE': mae,
                'RMSE': rmse,
                'MAPE': mape
            }

            print(f"Ensemble (Stacking) - MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}%")
            return meta_model, ensemble_pred

        else:
            print(f"Ensemble type {ensemble_type} not recognized. Skipping.")
            return None, None

def run_forecasting_pipeline(city=None, test_size=0.2, save_models=True):
    """
    Run the complete forecasting pipeline

    Parameters:
    -----------
    city : str, optional
        City to forecast for (default: use all cities)
    test_size : float, default=0.2
        Proportion of data to use for testing
    save_models : bool, default=True
        Whether to save trained models

    Returns:
    --------
    forecaster : ElectricityDemandForecaster
        Trained forecaster object
    """
    # Initialize forecaster
    forecaster = ElectricityDemandForecaster()

    # Load data
    data = forecaster.load_data()

    # Prepare features for specified city (or all cities)
    X, y = forecaster.prepare_features(city=city)

    # Split data
    X_train, X_test, y_train, y_test = forecaster.train_test_split_by_time(X, y, test_size=test_size)

    # Scale features
    X_train, X_test = forecaster.scale_features()

    # Create baseline forecast
    forecaster.create_baseline_forecast()

    # Train models
    forecaster.train_linear_regression()

    # Use smaller parameter grids for speed
    rf_params = {'n_estimators': [50], 'max_depth': [10, None]}
    forecaster.train_random_forest(param_grid=rf_params)

    gb_params = {'n_estimators': [50], 'learning_rate': [0.1]}
    forecaster.train_gradient_boosting(param_grid=gb_params)

    xgb_params = {'n_estimators': [50], 'learning_rate': [0.1]}
    forecaster.train_xgboost(param_grid=xgb_params)

    # Optionally train LSTM model
    try:
        forecaster.train_lstm(epochs=10, batch_size=32)
    except:
        print("Error training LSTM model. Skipping.")

    # If a specific city is provided, train SARIMA
    if city:
        try:
            forecaster.train_sarima(city)
        except:
            print(f"Error training SARIMA model for {city}. Skipping.")

    # Create ensemble
    forecaster.train_ensemble(ensemble_type='averaging')
    forecaster.train_ensemble(ensemble_type='stacking')




    return forecaster

# Example of how to use the functionality to analyze clustering and forecasting
def run_electricity_analysis_pipeline(data_path='processed_energy_data.csv', city=None):
    """
    Run the complete electricity analysis pipeline with both clustering and forecasting

    Parameters:
    -----------
    data_path : str
        Path to the processed data file
    city : str, optional
        City to analyze (default: use all cities)
    """
    # Step 1: Initialize forecaster
    forecaster = ElectricityDemandForecaster(data_path=data_path)

    # Step 2: Load data
    data = forecaster.load_data()

    # Step 3: Perform clustering (would be implemented in a separate module)
    # This would include demand pattern clustering to identify typical daily/weekly patterns

    # Step 4: Run the forecasting pipeline
    forecaster = run_forecasting_pipeline(city=city, test_size=0.2, save_models=True)

    # Step 5: Generate summary report
    print("\n" + "="*50)
    print(" ELECTRICITY DEMAND ANALYSIS SUMMARY ")
    print("="*50)



    return forecaster

if __name__ == "__main__":
    # Run the analysis pipeline
    forecaster = run_electricity_analysis_pipeline(city='nyc')